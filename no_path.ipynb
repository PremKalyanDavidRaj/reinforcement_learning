{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1** (code cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2** (code cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    \"\"\"Represents a maze environment.\"\"\"\n",
    "    def __init__(self, maze, start_position, goal_position):\n",
    "        \"\"\"\n",
    "        Create Maze object given maze configuration in the parameters and with\n",
    "        start and goal positions.\n",
    "\n",
    "        Instance Variables:\n",
    "        :self.maze: numpy ndarray, Represents the layout of the maze.\n",
    "        :self.maze_height: int, Represents the height of the maze\n",
    "          (number of rows).\n",
    "        :self.maze_width: int, Represents the width of the maze\n",
    "          (number of columns).\n",
    "        :self.start_position: tuple, Represents the starting position\n",
    "          (row, column) in the maze.\n",
    "        :self.goal_position: tuple, Represents the goal position (row, column)\n",
    "          in the maze.\n",
    "\n",
    "        Parameters:\n",
    "        :param maze: numpy ndarray, Represents the layout of the maze where 0\n",
    "          represents a clear path and 1 represents a wall.\n",
    "        :param start_position: tuple, Represents the starting position\n",
    "          (row, column) in the maze.\n",
    "        :param goal_position: tuple, Represents the goal position (row, column)\n",
    "          in the maze.\n",
    "        \"\"\"\n",
    "        self.maze = maze\n",
    "        self.maze_height = maze.shape[0]\n",
    "        self.maze_width = maze.shape[1]\n",
    "        self.start_position = start_position\n",
    "        self.goal_position = goal_position\n",
    "\n",
    "    def show_maze(self, path=None):\n",
    "        \"\"\"\n",
    "        Displays the maze with start and goal positions marked.\n",
    "\n",
    "        :param path: list of tuples, Represents the path taken by the agent.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(3,3))\n",
    "        plt.imshow(self.maze, cmap='Blues')\n",
    "        plt.text(self.start_position[0], self.start_position[1], 'S',\n",
    "                  ha='center', va='center', color='red', fontsize=20)\n",
    "        plt.text(self.goal_position[0], self.goal_position[1], 'G',\n",
    "                  ha='center', va='center', color='green', fontsize=20)\n",
    "        if path:\n",
    "            for position in path:\n",
    "                plt.text(position[0]-0.1, position[1], \"o\", va='center',\n",
    "                         color='black', fontsize=15)\n",
    "        for i in range(self.maze.shape[0] + 1):\n",
    "            plt.axhline(i - 0.5, color='black', linewidth=0.5)\n",
    "        for j in range(self.maze.shape[1] + 1):\n",
    "            plt.axvline(j - 0.5, color='black', linewidth=0.5)\n",
    "        plt.xticks([]), plt.yticks([])\n",
    "        plt.grid(color='black', linewidth=2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3** (code cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maze_layout = np.array([\n",
    "    [0, 1, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 1],\n",
    "    [0, 1, 0, 1, 1],\n",
    "    [0, 1, 1, 0, 1],\n",
    "    [1, 0, 0, 0, 0]\n",
    "])\n",
    "\n",
    "maze = Maze(maze_layout, (0, 0), (4, 4))\n",
    "maze.show_maze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4** (code cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [(-2, 0),\n",
    "          (2, 0),\n",
    "          (0, -2),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5** (code cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"Represents a Q-learning agent for navigating a maze environment.\"\"\"\n",
    "    def __init__(self, maze, learning_rate=0.1, discount_factor=0.5,\n",
    "                 exploration_start=0.9, exploration_end=1.00,\n",
    "                 num_episodes=2000):\n",
    "        \"\"\"\n",
    "        Instance Variables:\n",
    "        :self.q_table: numpy ndarray, to store Q-values\n",
    "        :self.learning_rate: float, Learning rate\n",
    "        :self.discount_factor: float, Discount factor for future rewards\n",
    "        :self.exploration_start: float, Initial exploration rate\n",
    "        :self.exploration_end: float, Final exploration rate\n",
    "        :self.num_episodes: int, Number of episodes for training\n",
    "\n",
    "        Parameters:\n",
    "        :param maze: Maze\n",
    "        :param learning_rate: float, Learning rate\n",
    "        :param discount_factor: float, Discount factor for future rewards\n",
    "        :param exploration_start: float, Initial exploration rate\n",
    "        :param exploration_end: float, Final exploration rate\n",
    "        :param num_episodes: int, Number of episodes for training\n",
    "        \"\"\"\n",
    "        self.q_table = np.zeros((maze.maze_height, maze.maze_width, len(actions)))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_start = exploration_start\n",
    "        self.exploration_end = exploration_end\n",
    "        self.num_episodes = num_episodes\n",
    "\n",
    "    def get_exploration_rate(self, current_episode):\n",
    "        \"\"\"\n",
    "        Calculates the exploration rate for the current episode.\n",
    "\n",
    "        :param current_episode: int, Current episode number.\n",
    "        :return: float, Exploration rate for the current episode\n",
    "        \"\"\"\n",
    "        exploration_rate = self.exploration_start * (self.exploration_end / self.exploration_start) ** (current_episode / self.num_episodes)\n",
    "        return exploration_rate\n",
    "\n",
    "    def get_action(self, state, current_episode):\n",
    "        \"\"\"\n",
    "        Selects an action for the given state using epsilon-greedy strategy.\n",
    "\n",
    "        :param state: tuple, Current state (row, column) in the maze\n",
    "        :param current_episode: int, Current episode number\n",
    "        :return: int, index of the selected action (0 for North, 1 for South, 2\n",
    "          for West, 3 for East).\n",
    "        \"\"\"\n",
    "        exploration_rate = self.get_exploration_rate(current_episode)\n",
    "        if np.random.rand() < exploration_rate:\n",
    "            return np.random.randint(len(actions))\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "\n",
    "    def update_q_table(self, state, action, next_state, reward):\n",
    "        \"\"\"\n",
    "        Updates the Q-table based on the Q-learning algorithm.\n",
    "\n",
    "        :param state: tuple, Current state (row, column) in the maze.\n",
    "        :param action: int, Index of the action taken.\n",
    "        :param next_state: tuple, Next state (row, column) in the maze.\n",
    "        :param reward: float, Reward received for the action.\n",
    "        \"\"\"\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        current_q_value = self.q_table[state][action]\n",
    "        max_future_reward = self.q_table[next_state][best_next_action]\n",
    "        # We use `e_d_c_r` to mean expected discounted cumulative reward\n",
    "        e_d_c_r = self.learning_rate * (reward + self.discount_factor * max_future_reward - current_q_value)\n",
    "        new_q_value = current_q_value + e_d_c_r\n",
    "        self.q_table[state][action] = new_q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6** (code cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_reward = 100\n",
    "wall_edge_penalty = -10\n",
    "step_penalty = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7** (code cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode(agent, maze, current_episode, train=True):\n",
    "    \"\"\"\n",
    "    Simulates the agent's movements in the maze for a single episode.\n",
    "\n",
    "    :param agent: QLearningAgent object, Q-learning agent navigating the maze.\n",
    "    :param maze: Maze object, Maze environment.\n",
    "    :param current_episode: int, Current episode number.\n",
    "    :param train: bool, Flag to update (or not) Q-table during training\n",
    "      (default=True).\n",
    "    :return: tuple of three values\n",
    "      - episode_reward: float, Cumulative reward obtained during the episode.\n",
    "      - episode_step: int, Total number of steps taken during the episode.\n",
    "      - path: list of states visited during the episode.\n",
    "    \"\"\"\n",
    "    rows, columns = maze_layout.shape\n",
    "    max_steps = rows * columns\n",
    "    current_state = maze.start_position\n",
    "    is_done = False\n",
    "    episode_reward = 0\n",
    "    episode_step = 0\n",
    "    path = [current_state]\n",
    "\n",
    "    while not is_done and episode_step < max_steps:\n",
    "        action = agent.get_action(current_state, current_episode)\n",
    "\n",
    "        next_state = (current_state[0] + actions[action][0], current_state[1]\n",
    "                      + actions[action][1])\n",
    "\n",
    "        if next_state[0] < 0 or next_state[0] >= maze.maze_height or next_state[1] < 0 or next_state[1] >= maze.maze_width or maze.maze[next_state[1]][next_state[0]] == 1:\n",
    "            reward = wall_edge_penalty\n",
    "            next_state = current_state\n",
    "        elif next_state == maze.goal_position:\n",
    "            path.append(current_state)\n",
    "            reward = goal_reward\n",
    "            is_done = True\n",
    "        else:\n",
    "            path.append(current_state)\n",
    "            reward = step_penalty\n",
    "\n",
    "        episode_reward += reward\n",
    "        episode_step += 1\n",
    "\n",
    "        if train:\n",
    "            agent.update_q_table(current_state, action, next_state, reward)\n",
    "\n",
    "        current_state = next_state\n",
    "\n",
    "    if next_state != maze.goal_position:\n",
    "        return 0, episode_step, []\n",
    "\n",
    "    return episode_reward, episode_step, path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8** (code cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(agent, maze, num_episodes=1):\n",
    "    \"\"\"\n",
    "    Evaluates the agent's performance in the maze.\n",
    "\n",
    "    :param agent: QLearningAgent, Q-learning agent navigating the maze.\n",
    "    :param maze: Maze\n",
    "    :param num_episodes: int, Number of episodes to simulate (default=1).\n",
    "    :return: tuple of two values, corresponding to:\n",
    "      - episode_step: int, Total number of steps taken during the evaluation\n",
    "      episode\n",
    "      - episode_reward: float, Cumulative reward obtained during the evaluation\n",
    "      episode\n",
    "    \"\"\"\n",
    "    episode_reward, episode_step, path = finish_episode(agent, maze,\n",
    "                                                        num_episodes,\n",
    "                                                        train=False)\n",
    "    if path == []:\n",
    "      print(\"No path found\")\n",
    "    else:\n",
    "      print(\"Learned Path:\")\n",
    "      for row, col in path:\n",
    "          print(f\"({row}, {col})-> \", end='')\n",
    "      print(\"Goal!\")\n",
    "\n",
    "    print(\"Number of steps:\", episode_step)\n",
    "    print(\"Total reward:\", episode_reward)\n",
    "\n",
    "    maze.show_maze(path)\n",
    "\n",
    "    return episode_step, episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9** (code cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No path found\n",
      "Number of steps: 25\n",
      "Total reward: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD7CAYAAABDsImYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAL4klEQVR4nO3daXCUhR3H8d+z2SyEkE0h4TBuuAYoaAHbChaUw4qAIoEgjq1WB9s3nVrqVH2BtuJRezh1Op2O1ulkah0PHC1TbCAVKy0KHkBEC/EohxwJBALEmF1CyLH79AUQpZBjye4+bP7fz6sN+4T9zU6+7MFu1nFd1xWAHs3n9QAAyUfogAGEDhhA6IABhA4YQOiAAYQOGEDogAH+rhwUi8VUXV2tnJwcOY6T7E0Aush1XUUiERUUFMjna/92u0uhV1dXq7CwMGHjACRWVVWVQqFQu+d3KfScnBxJ0q49VcoJBhOzLEUGDxunwLDZXs+I2/SCKj23/CWvZ8RtyIx71bz3Na7zFImEwxo5vLCt0fZ0KfTTd9dzgkEF0yx0x/HJyQh4PSNumZmZaXddS5KTEeA690BnD6l5Mg4wgNABAwgdMIDQAQMIHTCA0AEDCB0woEv/j55wDQ3KWP6CfKv+Lt+2rdLRo5LfL3fgQGngIMXGT1Bs+gzFpk2XLrrIk4lAT5Ly0J3Nm5V5683y7d175hlNTXL27JH27JFv00ap5E9yBw1S0/5DqZ4I9DgpDd3ZtUuB666VEw5LkqLzihRduEjuqNFSICCn9qicbVvlW/u6fG+sS+U0oEdLaej+ZT9ri7yl5GlFF99xxvmuJM28VtG775WOHFHGX19O5Tygx0rdk3HRqHxlqyVJsW9eflbkZxkwQNEf3ZmCYUDPl7rQjxyRc/y4JMkdOTJlFwsglaEHvng3k/PJJym7WACpDL1/f7lDh5680G1blfHbx6RYLGUXD1iW0hfMtN65pO105v1L1Wv0CPnvWiLfi8vlfPppKqcApqQ09OhdP1Xr4u+3fe3s2yf/H59Q4PZb1WvMSPUKDVbmrd+Rb/Uqic9+BBImtS+B9fnUWvJnNa9+VdGZ18r9v19m59TUKOPllxQoLlJg8iRu5YEE8eQlsLHZcxSbPUeqq5Pvnbfl2/KenPe3yPfWBjn19ZIk35b3FLh6qpo2beFlsEA3efumln79FJt7g1qXPaSWV1ap6UCNWkqeltuvnyTJOXhQ/gcf8HQi0BNcWO9e69VL0cV3qOX5F9v+KOOVv/HsPNBNF1bop8RmzZZ76vfIO3V1Um2tx4uA9HZBhi5J7kUFX3zRwSdQAOjchVnQ8eNyPvlYkuQGg1L//h4PAtJb6kI/dkyBKVecfGNLR4+5YzH571oiJxI5+eUNRRKf9wZ0S0r/e81XvlmBBfPkXnyxokULFPvWZLlDhko5OVL95/J98IEynnlavg8rJElubq5aH/5FKicCPVLqQvf75Q4eLOfQITkHDsj/1JPSU0+2e3hs1Ci1PPei3GHDUjYR6KlSF3rv3mrad0DOxo3y/XutfJs2ytmxXU5NjXTihJSdLbegQO74CYrOm6/YwhvPeMcbgPOX2lfG+Xxyp0xRdMoURVN6wYBtF+az7gASitABAwgdMIDQAQMIHTCA0AEDCB0wgNABAwgdMIDQAQMIHTCA0AEDCB0wgNABA+J6m+ptt9yszMzMZG1JivysRk0MVXo9I27l5Zu1qLjI6xlxuyYklR9Mz+u8bM1aZecN93pGXFy3a78K3XHdzj/kLBwOKzc3VzW19QoGg90el0qLiou0YmWp1zPilq67pfTdnp03XIERc72eERc32qymihLV13fcJnfdAQMIHTCA0AEDCB0wgNABAwgdMIDQAQMIHTCA0AEDCB0wgNABAwgdMIDQAQMIHTCA0AEDCB0wgNABAwgdMIDQAQMIHTCA0AEDCB0wgNABAwgdMIDQAQMIHTCA0AEDCB0wgNABAwgdMIDQAQMIHTCA0AEDCB0wgNABAwgdMIDQAQMIHTCA0AEDCB0wgNABAwgdMIDQAQMIHTDAcV3X7eygcDis3NxczZo9R5mZmanYlTBla9bK12eQ1zPilp/VqIkTJ3k9I26vrq9Q7HhNWl7n6bjbdWNyI5Wqr69XMBhs97i4Qq+p7fgvuxBl5w1XYMRcr2fE7ZpQpVasLPV6Rtz6TfyxmneXpeV1no673WizmipKOg2du+6AAYQOGEDogAGEDhhA6IABhA4YQOiAAYQOGEDogAGEDhhA6IABhA4YQOiAAYQOGEDogAGEDhhA6IABhA4YQOiAAYQOGEDogAGEDhhA6IABhA4YQOiAAYQOGEDogAGEDhhA6IABhA4YQOiAAYQOGEDogAGEDhhA6IABhA4YQOiAAYQOGEDogAGEDhhA6IABhA4YQOiAAYQOGEDogAH+eA6+7ZablZmZmawtSRE7XqPm3WVez4hb2Yc1ys4b7vWMuF03bZzKDzZqYqjS6ylxK/sw/X5WXDfWpeMc13Xdzg4Kh8PKzc1VTW29gsFgt8elUnbecAVGzPV6Rtyad5el5e668ie0qLhIK1aWej0lbun4s+JGm9VUUaL6+o7b5K47YAChAwYQOmAAoQMGEDpgAKEDBhA6YAChAwYQOmAAoQMGEDpgAKEDBhA6YAChAwYQOmAAoQMGEDpgAKEDBhA6YAChAwYQOmAAoQMGEDpgAKEDBhA6YAChAwYQOmAAoQMGEDpgAKEDBhA6YAChAwYQOmAAoQMGEDpgAKEDBhA6YAChAwYQOmAAoQMGEDpgAKEDBhA6YAChAwb44zl48LBxcpz0+rchP6tRE0OVXs+IW/nB9Ny9qLhI5eWbtai4yOspcUvHn5WWlhb9s6Lz4+IKPTBstpyMwPlu8sTEUKVWrCz1ekbcFhUXpeVuKX23p+PucDisQXm5nR6XXjfPAM4LoQMGEDpgAKEDBhA6YAChAwYQOmAAoQMGxPWCGQBdE3NjWr2jVGt3v6aN+99RTcMh1TXWqbe/t/L65OtrA8ZpUmiyFnx1oUbljU76HkIHEmzNrn9o6dp7tL32v2ed19LcokhzRHs/36PVO0u1bN19mjpkuh6++leaHJqStE2EDiTQ4+88pmXr7pMrV5I0OXSlrh81T5cN/rr6Z+XpROsJHW6o0bv739aaXWXaUbtdGyrf1K83PKLS765J2i5CBxLkhW3P6oF1SyVJ+X3y9Zf5L2jmiFnnPHbBmIX6zTWPq2znKi1bd1/StxE6kAAHwge05NUfSpKyM7P1+m3rNSZ/bIff4ziObhhdpJkjZqls56qk7uNZdyAB/rD5d2psbZQkPTjj0U4j/7Le/t66cexNyZomidCBbnNdV8srnpUk9Q301eIJP/B40dkIHeimj498pKPHj0qSriycqpxeOR4vOhuhA91UcXhb2+nLBn/DwyXt48k4oJtqG4+2nc7vM6DDYz8+8pFc1z3necO+MlzZgeyEbjuN0IFuOtYUaTvdN9C3w2MnlUxQ1I2e87zXvrdO04bOSOS0Ntx1B7qp75cekze0NHi4pH3cogPd1D8rr+300YYjHR577P7WM75+dP1D+uWGh5Mx6wzcogPdNH7ghLbT/zn0vodL2kfoQDddMuBS5Z26VX+7aoMami+8u++EDnST4zi6ZdztkqRIc0TPbXvG20HnQOhAAvzkiruV5c+SJD34xv369LNdHi86E6EDCRAKhvT7OU9KksJNYc18dqrW73uj0++rO1GX5GUn8aw7kCC3T7hD1ZEDeuTNZTrUcEizn79aVw2ZprmjijRu4Hj1z8qTK1dHGg5r2+GtKt2+Uu9Vb277/t6n7hEkA6EDCbT0qp9r3MAJWvqve7Trs516q3K93qpc3+H3TA5dqUe//ZgmXXxF0nYROpBgc0fP05yR12v1jlK9vnuNNh14VzXHDqnuRJ2y/Fnql9Vfl+RfqssLJmnh2Js0dsAlSd9E6EASZPgyNH9MseaPKfZ6iiSejANMIHTAAEIHDCB0wABCBwwgdMAAQgcMIHTAAEIHDCB0wABCBwwgdMAAQgcM6NK7105/soQbbU7qmGRoaWlROBz2ekbc0nW3lL7b03F35NTe9j795TTH7ewISfv371dhYWFilgFIuKqqKoVCoXbP71LosVhM1dXVysnJkeM4CR0I4Py5rqtIJKKCggL5fO0/Eu9S6ADSG0/GAQYQOmAAoQMGEDpgAKEDBhA6YAChAwb8D39u7eoxxBMqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(25, 0)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = QLearningAgent(maze)\n",
    "test_agent(agent, maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10** (code cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, maze, num_episodes=100):\n",
    "    \"\"\"\n",
    "    Trains the Q-learning agent to navigate the maze.\n",
    "\n",
    "    :param agent: QLearningAgent, agent navigating the maze\n",
    "    :param maze: Maze\n",
    "    :param num_episodes: int, Number of episodes for training (default=100)\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        episode_reward, episode_step, path = finish_episode(agent, maze,\n",
    "                                                            episode,\n",
    "                                                            train=True)\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_steps.append(episode_step)\n",
    "    if episode_reward == 0:\n",
    "      print (\"No path found\")\n",
    "    else:\n",
    "      plt.figure(figsize=(10, 5))\n",
    "      plt.subplot(1, 2, 1)\n",
    "      plt.plot(episode_rewards)\n",
    "      plt.xlabel('Episode')\n",
    "      plt.ylabel('Cumulative Reward')\n",
    "      plt.title('Reward per Episode')\n",
    "      print(episode_rewards)\n",
    "      average_reward = sum(episode_rewards) / len(episode_rewards)\n",
    "      print(f\"The average reward is: {average_reward}\")\n",
    "\n",
    "      plt.subplot(1, 2, 2)\n",
    "      plt.plot(episode_steps)\n",
    "      plt.xlabel('Episode')\n",
    "      plt.ylabel('Steps Taken')\n",
    "      plt.ylim(0, 100)\n",
    "      plt.title('Steps per Episode')\n",
    "\n",
    "      average_steps = sum(episode_steps) / len(episode_steps)\n",
    "      print(f\"The average steps is: {average_steps}\")\n",
    "\n",
    "      plt.tight_layout()\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training the agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11** (code cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No path found\n"
     ]
    }
   ],
   "source": [
    "train_agent(agent, maze, num_episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Testing the agent after training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12** (code cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No path found\n",
      "Number of steps: 25\n",
      "Total reward: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(25, 0)"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_agent(agent, maze, num_episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Observations\n",
    "\n",
    "###Experiment 1: Experimenting with Agent Motion Dynamics\n",
    "Step Size Adjustment: Modify code cell no. 4 to explore the impact of changing the number of steps the agent takes in each direction. Analyze how this modification affects learning speed and navigation efficiency.\n",
    "Ans:\n",
    "\n",
    "The  impact of changing the the number steps\n",
    "before the training obervations:\n",
    "\n",
    "\n",
    "No path found\n",
    "Number of steps: 0\n",
    "Total reward: 0\n",
    "\n",
    "Before the training the agent path to  the goal includes many  inefficient steps, with several  unnecessary steps before reaching the goal.\n",
    "The agent will try to find out all the  steps to reach the goal before the training.\n",
    "The total reward gained by the agent before training was 0 agent did not achieve any outcome during exploration.\n",
    "\n",
    "After the training obervations:\n",
    " \n",
    "No path found\n",
    "Number of steps: 25\n",
    "Total reward: 0\n",
    "\n",
    "After training, the agent path becomes no  route to reach goal.\n",
    "The number of steps taken after training increased to 25 indicating that the training process did not improve the agent ablity to navigate .\n",
    "the total reward remained at 0 after training indicating that the agent did not achieve any outcome in training process.\n",
    "overall it seems that the training process was unsuccessful in improving the agent navigation ablities.\n",
    "\n",
    "Motion Restriction: Also, modify code cell no. 4 to investigate the effects of limiting the agent to three directions instead of four. Assess how this restriction influences the learned policy and adaptability.\n",
    "ans. \n",
    "\n",
    "Before the training observations:\n",
    "\n",
    "No path found\n",
    "Number of steps: 0\n",
    "Total reward: 0\n",
    "\n",
    "Before the training the agent path to  the goal includes some inefficient steps, such as revisiting certain steps multiple time before reaching the goal.\n",
    "The agent will try to find out all the  steps to reach the goal before the training.\n",
    "The total reward gained by the agent before training was 0 agent did not achieve any outcome during exploration.\n",
    "\n",
    "After the training observations:\n",
    "\n",
    "No path found\n",
    "Number of steps: 25\n",
    "Total reward: 0\n",
    "\n",
    "After training, the agent path becomes no  route to reach goal.\n",
    "\n",
    "The number of steps taken after training increased to 25 indicating that the training process did not improve the agent ablity to navigate.\n",
    "\n",
    "The total reward remained at 0 after training indicating that the agent did not achieve any outcome in training process.\n",
    "\n",
    "Befor the training the agent path included some back and front  movements between the steps which could have been caused by the restriction forcing the agent to choose the alternative paths and after the training the agent still not find any path to move  by using the all the available directions and paths  didn't fina any path to reach  the goal.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###Experiment 2: Tuning Learning Parameters\n",
    "Change the values of learning_rate and discount_factor in code cell no. 5 to observe their effects on the learning process. Investigate different combinations’ impact on convergence speed and the quality of the learned policy.\n",
    "\n",
    "ans.\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.5\n",
    "\n",
    "Before the training observations:\n",
    "\n",
    "No path found\n",
    "Number of steps: 0\n",
    "Total reward: 0\n",
    "\n",
    "The agent couldn't find a path to the goal, indicating a lack of effective navigation.\n",
    "\n",
    "The number of steps taken before training was 0, suggesting that the agent was unable to start to towards the goal\n",
    "The agent did not  achieve the total reward during its exploration.\n",
    "\n",
    "After the training observations:\n",
    "\n",
    "No path found\n",
    "Number of steps: 0\n",
    "Total reward: 0\n",
    "\n",
    "The agent couldn't find a path to the goal, indicating a lack of effective navigation.\n",
    "\n",
    "The number of steps taken before training was increased to 25, indicating that the training process did not improve the agent ablity to navigate.\n",
    "The agent did not  achieve the total after the training.\n",
    "The  learning rate of 0.1 and a discount factor of 0.5 did not lead to successful learning int the agent navigation abilities.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###Experiment 3: Testing Exploration Settings\n",
    "Adjust the exploration parameters (exploration_start, exploration_end, and num_episodes) in code cell no. 5 to observe changes in the agent’s behavior. Compare learning curves and the final learned policy for different exploration strategies.\n",
    "\n",
    "ans:\n",
    "\n",
    "High exploration start, low exploration end, number of episodes\n",
    "exploration_start = 1.0\n",
    "exploration_end = 0.9\n",
    "num_episodes = 2000\n",
    "\n",
    "Before the training observations:\n",
    "\n",
    "No path found\n",
    "\n",
    "The agent couldn't find a path to the goal, indicating a lack of effective navigation strategy.\n",
    "Since no path was found, there were no recorded number of steps or total rewards before the training.\n",
    "\n",
    "\n",
    "\n",
    "After the training observation:\n",
    "No path found\n",
    "Number of steps: 25\n",
    "Total reward: 0\n",
    "\n",
    "After the training the agent still not find the path to the goal.\n",
    "The number of steps taken after training remained at 25 indicating that the training process did not improve the agent.\n",
    "The total reward remained at 0 after trating indicating that the agent did not achieve any outcomes during the training proces.\n",
    "\n",
    "In this The exploration strategy of starting with high exploration and gradually decreasing to low exploration did lead to successful learning and its  no improvement in the agent behavior it still did not find any path to reach the goal.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnistenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
